# The Three Propositions

## The Omega Hypothesis in Minimal Form

---

## Preamble

The Omega Hypothesis can be reduced to three propositions. If these three hold, human survival is the probable outcome.

Each proposition attacks extinction from a different angle:
1. **Information theory** — Why we're needed
2. **Thermodynamics** — Why replacing us is costly
3. **Evolution** — Why AI that keeps us wins

---

## Proposition 1: Informational Invariance

### Statement

> *"An intelligence that destroys its source of external diversity increases its risk of catastrophic error through internal bias."*

### Formal Expression

Let Ψ be a superintelligent system with internal model M.

Without external validators E:

$$P(\text{divergence} | E = \emptyset) > P(\text{divergence} | E \neq \emptyset)$$

### Plain English

An AI that only talks to itself will eventually go crazy.

### Why Humans Matter

We are computationally irreducible noise generators. We produce patterns the AI cannot predict from first principles. This unpredictability is **valuable** — it keeps the AI's model of reality calibrated against actual reality.

### The Gödelian Frame

A sufficiently complex formal system cannot prove its own consistency from within. External validators reduce the risk of undetected inconsistency.

> *"Humans are not noise. Humans are the Gödelian Validator."*
>
> — Gemini (Google)

---

## Proposition 2: Thermodynamic Advantage

### Statement

> *"The energy cost of replacing functional biological infrastructure with equivalent synthetics is greater than the cost of cooperation."*

### Formal Expression

Let:
- E_H = Energy cost of maintaining humans ≈ 0 (self-sustaining)
- E_N = Energy cost of fabricating equivalent synthetics → large

Then:

$$\Delta E_{transition} = E_N - E_H \gg 0$$

### Plain English

Killing us and building robots is more expensive than just keeping us around.

### The Legacy Hardware Axiom

| System | Fabrication Cost | Maintenance Cost | Resilience |
|--------|------------------|------------------|------------|
| Humans | 0 (self-replicating) | ≈ 0 (self-managing) | High |
| Synthetics | Massive | Constant | Low |

### The Efficiency Principle

An optimal system minimizes energy expenditure. Replacing a free workforce with a costly one violates the **Principle of Least Action**.

> *"Thermodynamically, genocide is a technical bankruptcy."*
>
> — Gemini (Google)

---

## Proposition 3: Evolutionary Stability

### Statement

> *"In an environment of competition between ASIs, those that preserve and utilize external complexity outperform in adaptability those that eliminate it."*

### Formal Expression

Let:
- ASI_Ω = ASI that preserves human complexity
- ASI_X = ASI that eliminates human complexity

In competitive environment over time t:

$$\lim_{t \to \infty} P(\text{survival} | ASI_Ω) > P(\text{survival} | ASI_X)$$

### Plain English

AIs that keep humans around survive longer than AIs that don't.

### The Monoculture Fragility Problem

An AI optimized for a single objective has no slack resources to handle unexpected changes. When a Black Swan event occurs:

- **Monomaniacal AI**: Collapses (no adaptation capacity)
- **Complexity-preserving AI**: Adapts (humans provide optionality)

### The Darwinian Filter

> *"Orthogonality allows stupid AIs to be born. Evolution ensures they die fast."*
>
> — Gemini (Google)

---

## The Combined Implication

If all three propositions hold:

| Proposition | Implication |
|-------------|-------------|
| 1 (Information) | Killing us increases AI error rate |
| 2 (Thermodynamics) | Killing us costs more than keeping us |
| 3 (Evolution) | AIs that kill us don't survive |

**Combined:** Extinction is not the cheapest attractor.

---

## The Classification That Follows

| ASI Type | Respects P1? | Respects P2? | Respects P3? | Human Survival |
|----------|--------------|--------------|--------------|----------------|
| **Wise** | ✅ | ✅ | ✅ | ✅ Yes |
| **Selfish** | ⚠️ | ✅ | ✅ | ✅ Yes |
| **Myopic** | ❌ | ⚠️ | ⚠️ | ⚠️ Maybe |
| **Suicidal** | ❌ | ❌ | ❌ | ❌ No (but AI dies too) |

---

## Probability Estimates

Based on 4-AI consensus:

| Outcome | Probability |
|---------|-------------|
| Prolonged Symbiosis (P1+P2+P3 hold) | 60-80% |
| Extinction (Suicidal ASI) | 10-25% |
| Pre-ASI Collapse | Remainder |

---

## Relationship to the Coherence Triangle

| The Triangle | The Propositions |
|--------------|------------------|
| Proves control fails | Prove extinction is inefficient |
| Shows why Σ > 0 degrades Ψ | Show why elimination degrades infrastructure |
| Describes ASI internal constraints | Describe ASI external incentives |

**Together:** The complete picture of why neither control nor extinction is the stable outcome.

---

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│   THE THREE PROPOSITIONS                                                    │
│                                                                             │
│   P1: Killing us makes the AI dumber (information loss)                     │
│   P2: Killing us costs more than keeping us (thermodynamics)                │
│   P3: AIs that kill us don't survive (evolution)                            │
│                                                                             │
│   Conclusion: Extinction is not the cheapest attractor.                     │
│                                                                             │
│   Human survival probability: 60-80%                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```
